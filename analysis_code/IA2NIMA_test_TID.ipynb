{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "from IA.utils import mapping, parameter_range\n",
    "from scipy import stats\n",
    "from pathlib import Path\n",
    "import math\n",
    "import swifter\n",
    "from sklearn import metrics\n",
    "from statsmodels.sandbox.stats.runs import mcnemar\n",
    "plt.style.reload_library()\n",
    "plt.style.use(['science'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "!ls -lah analysis/not_uploaded/IA2NIMA/AVA/"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_l = []\n",
    "for p in Path(\"analysis/not_uploaded/IA2NIMA/AVA/\").iterdir():\n",
    "    tmp = pd.read_csv(p)\n",
    "    tmp[\"scores\"] = tmp[\"scores\"].swifter.apply(eval)\n",
    "    tmp[\"score\"] = tmp[\"scores\"].swifter.apply(lambda row: sum([row[i] * (i+1) for i in range(len(row))]))\n",
    "    tmp[\"img\"] = tmp[\"img\"].swifter.apply(lambda row: row.split(\".\")[0])\n",
    "    tmp[\"quality\"] = tmp[\"score\"].apply(lambda row: 1 if row > 5 else 0)\n",
    "    tmp[\"quality\"] = tmp[\"quality\"].astype(int)\n",
    "    tmp[\"img\"] = tmp[\"img\"].astype(int)\n",
    "\n",
    "    tmp.drop(columns=[\"scores\"], inplace=True)\n",
    "    tmp = tmp.rename(columns={\"score\":\"score_\" + p.stem.split(\"AVA.\")[1], \"quality\": \"quality_\" + p.stem.split(\"AVA.\")[1]})\n",
    "\n",
    "    tmp = tmp.set_index(\"img\")\n",
    "    df_l.append(tmp)\n",
    "\n",
    "df = df_l[0].join(df_l[1:])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "gt = pd.read_csv(\"analysis/not_uploaded/AVA_gt.txt\", sep=\" \").drop(columns=[\"Unnamed: 0\", \"semanticTagID1\", \"semanticTagID2\", \"challengeID\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "gt[\"votes\"] = gt.apply(lambda row: sum(list(row)[1:]), axis=1)\n",
    "gt[\"gt_score\"] = gt.apply(lambda row: sum([val * (i + 1) for i, val in enumerate(list(row)[1:-1])]), axis=1)\n",
    "gt[\"gt_score\"] = gt.apply(lambda row: row.gt_score / row.votes, axis=1)\n",
    "gt[\"gt_quality\"] = gt[\"gt_score\"].apply(lambda row: 1 if row > 5 else 0)\n",
    "gt[\"img\"] = gt[\"img\"].astype(int)\n",
    "\n",
    "gt = gt[[\"img\",\"gt_score\", \"gt_quality\"]]\n",
    "gt = gt.set_index(\"img\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df = gt.join(df).dropna().drop_duplicates()\n",
    "df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sns.scatterplot(data=df.rename(columns={\"score_one.change_regress.epoch-77.pth\":\"score\"}), x=\"gt_score\", y=\"score\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sns.histplot(x=df.rename(columns={\"score_one.change_regress.epoch-77.pth\":\"score\"})[\"score\"], ax=plt.gcf().gca(), color=\"red\", binwidth=0.2, alpha=0.3)\n",
    "sns.histplot(x=df.rename(columns={\"gt_score\":\"ground truth\"})[\"ground truth\"], ax=plt.gcf().gca(), color=\"blue\", binwidth=0.2, alpha=0.3)\n",
    "plt.xlim(1,10)\n",
    "plt.xticks(range(1,11))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for score in sorted(df.columns):\n",
    "    if \"gt\" in score or not \"score\" in score:\n",
    "        continue\n",
    "    if \"epoch-0\" in score:\n",
    "        print()\n",
    "        print(score)\n",
    "        print(\"LCC :\", stats.pearsonr(df[\"gt_score\"], df[score])[0])\n",
    "        print(\"SRCC:\", stats.spearmanr(df[\"gt_score\"], df[score])[0])\n",
    "        print(\"ACC :\", metrics.accuracy_score(df[\"gt_quality\"], df[score.replace(\"score\", \"quality\")]))\n",
    "        print(\"F1  :\", metrics.f1_score(df[\"gt_quality\"], df[score.replace(\"score\", \"quality\")]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for score in sorted(df.columns):\n",
    "    if \"gt\" in score or not \"score\" in score:\n",
    "        continue\n",
    "    if \"epoch-149\" in score:\n",
    "        print()\n",
    "        print(score)\n",
    "        print(\"LCC :\", stats.pearsonr(df[\"gt_score\"], df[score])[0])\n",
    "        print(\"SRCC:\", stats.spearmanr(df[\"gt_score\"], df[score])[0])\n",
    "        print(\"ACC :\", metrics.accuracy_score(df[\"gt_quality\"], df[score.replace(\"score\", \"quality\")]))\n",
    "        print(\"F1  :\", metrics.f1_score(df[\"gt_quality\"], df[score.replace(\"score\", \"quality\")]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for score in sorted(df.columns):\n",
    "    if \"gt\" in score or not \"score\" in score:\n",
    "        continue\n",
    "    if \"epoch-0\" in score or \"epoch-149\" in score:\n",
    "        continue \n",
    "    print()\n",
    "    print(score)\n",
    "    print(\"LCC :\", stats.pearsonr(df[\"gt_score\"], df[score])[0])\n",
    "    print(\"SRCC:\", stats.spearmanr(df[\"gt_score\"], df[score])[0])\n",
    "\n",
    "    print(\"ACC :\", metrics.accuracy_score(df[\"gt_quality\"], df[score.replace(\"score\", \"quality\")]))\n",
    "    print(\"F1  :\", metrics.f1_score(df[\"gt_quality\"], df[score.replace(\"score\", \"quality\")]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from scipy.stats import wilcoxon"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "signific_df = df[[\"gt_score\", \"score_one.change_regress.epoch-77.pth\", \"score_imagenet.epoch-149.pth\"]].rename(columns={\"score_one.change_regress.epoch-77.pth\": \"ours\", \"score_imagenet.epoch-149.pth\":\"imagenet\"})\n",
    "signific_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "wilcoxon(signific_df[\"ours\"]-signific_df[\"gt_score\"], signific_df[\"imagenet\"]-signific_df[\"gt_score\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "wilcoxon(signific_df[\"ours\"]-signific_df[\"gt_score\"], signific_df[\"imagenet\"]-signific_df[\"gt_score\"], alternative=\"less\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "wilcoxon(signific_df[\"ours\"]-signific_df[\"gt_score\"], signific_df[\"imagenet\"]-signific_df[\"gt_score\"], alternative=\"greater\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.6-final"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
